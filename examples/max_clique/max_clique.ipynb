{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code example of Symbolic Sampling for Max-Clique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure autoqubo is also installed in this jupyter kernel\n",
    "%cd ../..\n",
    "%pip install -e .\n",
    "%cd examples/max_clique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "\n",
    "from autoqubo import SamplingCompiler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to parse Max-Clique instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make DIMACS files accessible\n",
    "DATASET_PATH = \"..//data/max_clique_dimacs_subset/\"\n",
    "\n",
    "\n",
    "instances_200 = [\n",
    "    \"c-fat200-1.clq\",\n",
    "    \"c-fat200-2.clq\",\n",
    "    \"brock200_1.clq\",\n",
    "    \"brock200_2.clq\",\n",
    "    \"brock200_3.clq\",\n",
    "    \"san200_0.7_1.clq\",\n",
    "    \"san200_0.7_2.clq\",\n",
    "    \"san200_0.9_1.clq\",\n",
    "    \"san200_0.9_2.clq\",\n",
    "    \"san200_0.9_3.clq\",\n",
    "]\n",
    "BATCH_SMALL = [Path().joinpath(DATASET_PATH + fn) for fn in instances_200]\n",
    "\n",
    "\n",
    "def read_graph(file_path):\n",
    "    \"\"\"reads edge set from DIMACS graph\n",
    "    Args:\n",
    "        file_path(os.PathLike): path to dimacs formatted textfile\n",
    "    Returns:\n",
    "        edges(np.ndarray): adjacency matrix,  edges[v1, v2]==1 iff v1, v2 are adjacent\n",
    "        vertices_num(int): number of vertices\n",
    "    \"\"\"\n",
    "    last_vertex_has_edge = False\n",
    "    with open(file_path, encoding=\"utf-8\") as graph_file:\n",
    "        for i, line in enumerate(graph_file):\n",
    "            if i == 0:\n",
    "                instance_name = str(Path(file_path).stem)\n",
    "                print(f\"Reading graph: {instance_name}\")\n",
    "            elif line.startswith(\"p\"):\n",
    "                _, _, vertices_num, edges_num = line.split()\n",
    "                vertices_num = int(vertices_num)\n",
    "                print(f\"Vertices: {vertices_num}, Edges: {edges_num}\")\n",
    "                edges = np.zeros(shape=(vertices_num, vertices_num))\n",
    "            elif line.startswith(\"e\"):\n",
    "                _, temp_1, temp_2 = line.split()\n",
    "                if temp_1 == temp_2:\n",
    "                    continue\n",
    "                # only save one pair (i,j) per edge, i<j\n",
    "                temp_1 = int(temp_1)\n",
    "                temp_2 = int(temp_2)\n",
    "                node_1 = min(temp_1, temp_2) - 1  # node names start with 1\n",
    "                node_2 = max(temp_1, temp_2) - 1\n",
    "                assert node_2 <= vertices_num, \"specified number of nodes is incorrect\"\n",
    "                if node_2 == vertices_num - 1:\n",
    "                    last_vertex_has_edge = True\n",
    "                edges[node_1, node_2] = 1\n",
    "            elif line.startswith(\"c\"):  # ignore comment lines\n",
    "                continue\n",
    "            else:\n",
    "                raise ValueError(\"unknown line in dimacs graph file: \", line)\n",
    "    if not last_vertex_has_edge:\n",
    "        raise ValueError(\n",
    "            \"number of vertices does not match edge set. last vertex has no edges.\"\n",
    "        )\n",
    "    return edges, vertices_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicit Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicit-only constraint function\n",
    "def old_constraint(x, edges):\n",
    "    \"\"\"returns number of constraint violations\"\"\"\n",
    "    sum_violations = 0\n",
    "    for i, j in combinations(range(len(x)), 2):\n",
    "        edge = edges[i, j]\n",
    "        if (x[i] == 1 and x[j] == 1) and edge == 0:\n",
    "            sum_violations += 1\n",
    "    return sum_violations\n",
    "\n",
    "def cost_function(x):\n",
    "    \"\"\"maximize clique size\"\"\"\n",
    "    return -1 * sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load example max clique instance\n",
    "filename = BATCH_SMALL[0]\n",
    "edges, n = read_graph(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicit constraint function defined by given graph\n",
    "explicit_constraint = lambda x: old_constraint(x, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain explicit QUBO\n",
    "qubo1, offset = SamplingCompiler.generate_qubo(\n",
    "    cost_function, explicit_constraint, n, penalty_method=\"sum\", use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUBO matrix only has numeric entries\n",
    "qubo1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbolic Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constraint function that can handle\n",
    "# both explicit and symoblic adjacency matrix\n",
    "def new_constraint(x, edges):\n",
    "    \"\"\"returns number of constraint violations\"\"\"\n",
    "    sum_violations = 0\n",
    "    for i, j in combinations(range(len(x)), 2):\n",
    "        edge = edges[i, j]\n",
    "        if (x[i] == 1 and x[j] == 1) and edge != 1:\n",
    "            sum_violations += 1 - edge\n",
    "    return sum_violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we could use this constraint\n",
    "# the same way as before, with an explicit\n",
    "# adjacency matrix\n",
    "explicit_constraint = lambda x: new_constraint(x, edges)\n",
    "# obtain explicit QUBO, same as above\n",
    "qubo2, offset = SamplingCompiler.generate_qubo(\n",
    "    cost_function, explicit_constraint, n, penalty_method=\"sum\", use_multiprocessing=False\n",
    ")\n",
    "assert (qubo1 == qubo2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols\n",
    "from autoqubo.symbolic import symbolic_matrix\n",
    "from autoqubo.symbolic import insert_values\n",
    "\n",
    "\n",
    "# create symbolic matrix\n",
    "# use example graph to determine number of symbolic variables\n",
    "symbolic_edges = symbolic_matrix(edges.shape[0], edges.shape[1])\n",
    "# constraint is only dependent on graph size\n",
    "constraint = lambda x: new_constraint(x, symbolic_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate symbolic QUBO\n",
    "symb_qubo, offset = SamplingCompiler.generate_qubo(\n",
    "    cost_function, constraint, n, penalty_method=\"sum\", use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbolic QUBO has formulas as entries\n",
    "symb_qubo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert explicit values to obtain explicit QUBO\n",
    "explicit_qubo = insert_values(symb_qubo, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explicit_qubo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result is same as QUBO from explicit sampling\n",
    "(explicit_qubo == qubo1).all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "DATASET = BATCH_SMALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbolic sampling\n",
    "start_symb = timer()\n",
    "# generate symbolic QUBO\n",
    "symb_qubo, offset = SamplingCompiler.generate_qubo(\n",
    "    cost_function, constraint, n, penalty_method=\"sum\", use_multiprocessing=False\n",
    ")\n",
    "# insert explicit values to obtain explicit QUBO\n",
    "for filename in DATASET:\n",
    "    edges, n = read_graph(filename)\n",
    "    explicit_qubo = insert_values(symb_qubo, edges)\n",
    "stop_symb = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicit sampling\n",
    "start_exp = timer()\n",
    "# sample for each explicit QUBO\n",
    "for filename in DATASET:\n",
    "    edges, n = read_graph(filename)\n",
    "    # explicit constraint function defined by given graph\n",
    "    explicit_constraint = lambda x: old_constraint(x, edges)\n",
    "    explicit_qubo, offset = SamplingCompiler.generate_qubo(\n",
    "    cost_function, explicit_constraint, n, penalty_method=\"sum\", use_multiprocessing=False\n",
    ")\n",
    "stop_exp = timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symb_time = np.round(stop_symb - start_symb, 2)\n",
    "exp_time = np.round(stop_exp - start_exp, 2)\n",
    "print(f\"Symbolic sampling took {symb_time}s in total.\")\n",
    "print(f\"Explicit sampling took {exp_time}s in total.\")\n",
    "print(f\"Symbolic sampling was {np.round(exp_time/symb_time, 2)}x faster for {len(DATASET)} instances.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoqubo",
   "language": "python",
   "name": "autoqubo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c5cbeb979e7f019a01b7133f5a9a19a8e92141f782ccb537b535a928e3b53eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
